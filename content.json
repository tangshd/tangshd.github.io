{"meta":{"title":"唐某某的blog","subtitle":"一切都是代码","description":"因为信任所以简单","author":"tangshd","url":"https://engining.net","root":"/"},"pages":[{"title":"404 Not Found：该页无法显示","date":"2022-08-03T01:33:24.587Z","updated":"2022-07-24T11:54:20.679Z","comments":false,"path":"/404.html","permalink":"https://engining.net/404.html","excerpt":"","text":""},{"title":"书单","date":"2022-07-24T12:01:54.747Z","updated":"2022-07-24T11:54:20.680Z","comments":false,"path":"books/index.html","permalink":"https://engining.net/books/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-07-24T12:01:54.654Z","updated":"2022-07-24T11:54:20.680Z","comments":false,"path":"tags/index.html","permalink":"https://engining.net/tags/index.html","excerpt":"","text":""},{"title":"Repositories","date":"2022-07-24T12:01:54.679Z","updated":"2022-07-24T11:54:20.680Z","comments":false,"path":"repository/index.html","permalink":"https://engining.net/repository/index.html","excerpt":"","text":""},{"title":"友情链接","date":"2022-07-24T12:01:54.710Z","updated":"2022-07-24T11:54:20.680Z","comments":true,"path":"links/index.html","permalink":"https://engining.net/links/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-07-24T12:01:54.728Z","updated":"2022-07-24T11:54:20.680Z","comments":false,"path":"categories/index.html","permalink":"https://engining.net/categories/index.html","excerpt":"","text":""},{"title":"关于我","date":"2018-09-26T11:11:14.000Z","updated":"2022-08-03T02:25:57.594Z","comments":false,"path":"about/index.html","permalink":"https://engining.net/about/index.html","excerpt":"","text":"我的公司： 上海凯京信达科技集团有限公司（“凯京集团”），，2015年成立于上海陆家嘴。 专注为中小微企业和个人提供各类场景下的信贷服务，产品包括商业保理业务、融资租赁业务、供应链金融、大数据征信等。 - 2015年获得知名投资机构数千万元人民币天使轮投资 - 2016年2月获得红杉资本领投的1亿元人民币A轮投资 - 2016年12月获得由中航信托领投、红杉资本、复朴资本跟投的2亿人民币B轮融资。 我们的使命： 数据重构物流 我们的愿景： 最具价值的数据科技企业"}],"posts":[{"title":"how to use hexo theme - pure","slug":"post-title-with-whitespace","date":"2022-07-24T04:54:53.000Z","updated":"2022-08-03T10:45:06.651Z","comments":true,"path":"2022/07/24/post-title-with-whitespace/","link":"","permalink":"https://engining.net/2022/07/24/post-title-with-whitespace/","excerpt":"","text":"","categories":[{"name":"hexo theme","slug":"hexo-theme","permalink":"https://engining.net/categories/hexo-theme/"}],"tags":[{"name":"pure","slug":"pure","permalink":"https://engining.net/tags/pure/"}]},{"title":"Hello Hexo","slug":"hello-world","date":"2022-07-22T07:54:53.000Z","updated":"2022-08-03T10:45:31.958Z","comments":true,"path":"2022/07/22/hello-world/","link":"","permalink":"https://engining.net/2022/07/22/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[{"name":"hexo","slug":"hexo","permalink":"https://engining.net/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://engining.net/tags/hexo/"},{"name":"quick start","slug":"quick-start","permalink":"https://engining.net/tags/quick-start/"}]},{"title":"mysql 双主同步","slug":"archive/2018-02-09-MySQL双主同步设置","date":"2021-09-27T03:01:02.448Z","updated":"2022-08-03T10:46:22.722Z","comments":true,"path":"2021/09/27/archive/2018-02-09-MySQL双主同步设置/","link":"","permalink":"https://engining.net/2021/09/27/archive/2018-02-09-MySQL%E5%8F%8C%E4%B8%BB%E5%90%8C%E6%AD%A5%E8%AE%BE%E7%BD%AE/","excerpt":"","text":"本文转载自https://github.com/sxyx2008/DevArticles Ubuntu 12.04.5 MySQL 5.5.47 master1 192.168.64.131 master2 192.168.64.132 1. 修改mysql配置文件、创建帐号并授权1.1 修改master1上mysql配置文件my.conf12345678910[mysqld]server-id = 131 #数据库IDlog_bin = /var/log/mysql/mysql-bin.log #启用二进制日志 如果没有var/log/mysql这个目录，则需要创建.#binlog-do-db = tudou1 #需要同步的数据库,这里同步tudou1和tudou2两个数据库#binlog-do-db = tudou2binlog-ignore-db = mysql #忽略同步的数据库log-slave-updates #把从库的写操作记录到binlog中 （缺少之后，双主创建失败）expire_logs_days = 365 #日志文件过期天数，默认是 0，表示不过期auto-increment-increment= 2 #设定为主服务器的数量，防止auto_increment字段重复auto-increment-offset = 1 #自增长字段的初始值，在多台master环境下，不会出现自增长ID重复 创建帐号密码并授权12GRANT REPLICATION SLAVE ON *.* TO &#x27;repuser&#x27;@&#x27;192.168.64.132&#x27; IDENTIFIED BY &#x27;repuser&#x27;;FLUSH PRIVILEGES; 在192.168.64.132测试repuser是否能登录192.168.64.131上的数据库1ubuntu@192.168.64.132:~/apps$ mysql -urepuser -prepuser -h192.168.64.131 1.2 修改master2上mysql配置文件my.conf12345678910[mysqld]server-id = 132 #数据库IDlog_bin = /var/log/mysql/mysql-bin.log #启用二进制日志 如果没有var/log/mysql这个目录，则需要创建.#binlog-do-db = tudou1 #需要同步的数据库,这里同步tudou1和tudou2两个数据库#binlog-do-db = tudou2binlog-ignore-db = mysql #忽略同步的数据库log-slave-updates #把从库的写操作记录到binlog中 （缺少之后，双主创建失败）expire_logs_days = 365 #日志文件过期天数，默认是 0，表示不过期auto-increment-increment= 2 #设定为主服务器的数量，防止auto_increment字段重复auto-increment-offset = 1 #自增长字段的初始值，在多台master环境下，不会出现自增长ID重复 创建帐号密码并授权12GRANT REPLICATION SLAVE ON *.* TO &#x27;repuser&#x27;@&#x27;192.168.64.131&#x27; IDENTIFIED BY &#x27;repuser&#x27;;FLUSH PRIVILEGES; 在192.168.64.131测试repuser是否能登录192.168.64.132上的数据库1ubuntu@192.168.64.131:~/apps$ mysql -urepuser -prepuser -h192.168.64.132 注意： log-slave-updates 表示把从库的写操作记录到binlog中，缺少之后，双主创建失败。双主同步时该项必须有 binlog-do-db 表示需要同步的数据库可出现多个，上述配置中注释掉了，若开启该配置项则格式见上述配置 binlog-ignore-db 表示忽略同步的数据库 2. 配置双主同步查看master状态master1中 1234567mysql&gt; show master status;+------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000001 | 107 | | mysql |+------------------+----------+--------------+------------------+1 row in set (0.00 sec) master2中 1234567mysql&gt; show master status;+------------------+----------+--------------+------------------+| File | Position | Binlog_Do_DB | Binlog_Ignore_DB |+------------------+----------+--------------+------------------+| mysql-bin.000001 | 107 | | mysql |+------------------+----------+--------------+------------------+1 row in set (0.00 sec) 设置master1从master2同步1234mysql&gt; CHANGE MASTER TO MASTER_HOST=&#x27;192.168.64.132&#x27;,MASTER_PORT=3306,MASTER_USER=&#x27;repuser&#x27;,MASTER_PASSWORD=&#x27;repuser&#x27;,MASTER_LOG_FILE=&#x27;mysql-bin.000001&#x27;,MASTER_LOG_POS=107;mysql&gt; SHOW SLAVE STATUS\\Gmysql&gt; START SLAVE;mysql&gt; SHOW SLAVE STATUS\\G 如出现以下两项，则说明配置成功！ 12Slave_IO_Running: YesSlave_SQL_Running: Yes 设置master2从master1同步1234mysql&gt; CHANGE MASTER TO MASTER_HOST=&#x27;192.168.64.131&#x27;,MASTER_PORT=3306,MASTER_USER=&#x27;repuser&#x27;,MASTER_PASSWORD=&#x27;repuser&#x27;,MASTER_LOG_FILE=&#x27;mysql-bin.000001&#x27;,MASTER_LOG_POS=107;mysql&gt; SHOW SLAVE STATUS\\Gmysql&gt; START SLAVE;mysql&gt; SHOW SLAVE STATUS\\G 如出现以下两项，则说明配置成功！ 12Slave_IO_Running: YesSlave_SQL_Running: Yes 3 双主同步测试进入master1 mysql 数据库 1234567891011121314151617181920212223242526mysql&gt; create database crm;Query OK, 1 row affected (0.00 sec)mysql&gt; use crm;Database changedmysql&gt; create table employee(id int auto_increment,name varchar(10),primary key(id));Query OK, 0 rows affected (0.00 sec)mysql&gt; insert into employee(name) values(&#x27;a&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into employee(name) values(&#x27;b&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; insert into employee(name) values(&#x27;c&#x27;);Query OK, 1 row affected (0.06 sec)mysql&gt; select * from employee;+----+------+| id | name |+----+------+| 1 | a || 3 | b || 5 | c |+----+------+3 rows in set (0.00 sec) 进入master2，查看是否有crm这个数据库和employee表。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647mysql&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || crm || mysql || performance_schema |+--------------------+4 rows in set (0.00 sec)mysql&gt; use crm;Reading table information for completion of table and column namesYou can turn off this feature to get a quicker startup with -ADatabase changedmysql&gt; show tables;+---------------+| Tables_in_crm |+---------------+| employee |+---------------+1 row in set (0.00 sec)mysql&gt; select * from employee;+----+------+| id | name |+----+------+| 1 | a || 3 | b || 5 | c |+----+------+3 rows in set (0.00 sec)mysql&gt; insert into employee(name) values(&#x27;d&#x27;);Query OK, 1 row affected (0.00 sec)mysql&gt; select * from employee;+----+------+| id | name |+----+------+| 1 | a || 3 | b || 5 | c || 7 | d |+----+------+4 rows in set (0.00 sec) 在master1的中查看是否有刚刚在master2中插入的数据。 12345678910 mysql&gt; select * from employee;+----+------+| id | name |+----+------+| 1 | a || 3 | b || 5 | c || 7 | d |+----+------+4 rows in set (0.00 sec)","categories":[],"tags":[{"name":"mysql master-master binlog","slug":"mysql-master-master-binlog","permalink":"https://engining.net/tags/mysql-master-master-binlog/"}]},{"title":"云原生、微服务、servicemesh的关系","slug":"archive/2018-12-25-云原生微服务servicemesh的关系","date":"2018-12-24T16:00:00.000Z","updated":"2021-09-27T03:01:02.448Z","comments":true,"path":"2018/12/25/archive/2018-12-25-云原生微服务servicemesh的关系/","link":"","permalink":"https://engining.net/2018/12/25/archive/2018-12-25-%E4%BA%91%E5%8E%9F%E7%94%9F%E5%BE%AE%E6%9C%8D%E5%8A%A1servicemesh%E7%9A%84%E5%85%B3%E7%B3%BB/","excerpt":"","text":"前言当下，云原生应用发展的如火如荼，各线下沙龙活动、线上知识文章也层出不穷？那什么是Cloud Native（云原生），它和微服务、容器化以及ServiceMesh有什么关系？ 先贴一张思维导图 参考资料：https://jimmysong.io http://www.servicemesher.com/ 容器 容器——Cloud Native的基石 K8S微服务Cloud Native（云原生）Service Mesh（服务网格）实际业务中的落地","categories":[],"tags":[{"name":"云原生","slug":"云原生","permalink":"https://engining.net/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"微服务","slug":"微服务","permalink":"https://engining.net/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"servicemesh","slug":"servicemesh","permalink":"https://engining.net/tags/servicemesh/"},{"name":"容器化","slug":"容器化","permalink":"https://engining.net/tags/%E5%AE%B9%E5%99%A8%E5%8C%96/"}],"author":"唐某某"},{"title":"阿里云日志服务采集k8s日志并实现livetail功能","slug":"archive/2018-11-15-阿里云日志服务采集k8s日志并实现livetail功能","date":"2018-11-14T16:00:00.000Z","updated":"2021-09-27T03:01:02.448Z","comments":true,"path":"2018/11/15/archive/2018-11-15-阿里云日志服务采集k8s日志并实现livetail功能/","link":"","permalink":"https://engining.net/2018/11/15/archive/2018-11-15-%E9%98%BF%E9%87%8C%E4%BA%91%E6%97%A5%E5%BF%97%E6%9C%8D%E5%8A%A1%E9%87%87%E9%9B%86k8s%E6%97%A5%E5%BF%97%E5%B9%B6%E5%AE%9E%E7%8E%B0livetail%E5%8A%9F%E8%83%BD/","excerpt":"","text":"前言目前的项目日志都是通过Logtail直接采集，投递到OSS持久化，同时可以通过阿里云日志服务、devops自建平台进行查看（虽然大部分人是直接登录ECS查看&#x3D;。&#x3D;）在开始进行容器化之后，同样遇到日志的问题，目前的解决方案是阿里云日志服务持久化和展现格式化后的日志、使用rancher查看实时日志但是之前由于rancher平台出现一些问题，导致不能及时查看日志的情况，在这个背景下对阿里云日志服务采集k8s日志和livetail进行搭建并调研词方案是否可行 简介（转自阿里云官方文档） 日志服务（Log Service，简称 LOG）是针对日志类数据的一站式服务，在阿里巴巴集团经历大量大数据场景锤炼而成。您无需开发就能快捷完成日志数据采集、消费、投递以及查询分析等功能，提升运维、运营效率，建立 DT 时代海量日志处理能力。 kubernetes日志采集组件安装安装Logtail 进入阿里云容器服务找到集群id 通过ssh登录master节点，或者任意安装了kubectl并配置了该集群kubeconfig的服务器 执行命令，将${your_k8s_cluster_id}替换为集群id 1wget http://logtail-release-cn-hangzhou.oss-cn-hangzhou.aliyuncs.com/kubernetes/alicloud-log-k8s-install.sh -O alicloud-log-k8s-install.sh; chmod 744 ./alicloud-log-k8s-install.sh; sh ./alicloud-log-k8s-install.sh $&#123;your_k8s_cluster_id&#125; Project k8s-log-$&#123;your_k8s_cluster_id&#125;下会自动创建名为config-operation-log的Logstore，用于存储alibaba-log-controller的运行日志。 请勿删除此Logstore，否则无法为alibaba-log-controller排查问题。 若您需要将日志采集到已有的Project，请执行安装命令sh ./alicloud-log-k8s-install.sh$&#123;your_k8s_cluster_id&#125; $&#123;your_project_name&#125; ，并确保日志服务Project和您的Kubernetes集群在同一地域。 该条命令其实就是执行了一个shell脚本，使用helm安装了采集kubernetes集群日志的组件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051 #!/bin/bash if [ $# -eq 0 ] ; then echo &quot;[Invalid Param], use sudo ./install-k8s-log.sh &#123;your-k8s-cluster-id&#125;&quot; exit 1fi clusterName=$(echo $1 | tr &#x27;[A-Z]&#x27; &#x27;[a-z]&#x27;)curl --connect-timeout 5 http://100.100.100.200/latest/meta-data/region-id if [ $? != 0 ]; then echo &quot;[FAIL] ECS meta server connect fail, only support alibaba cloud k8s service&quot; exit 1fi regionId=`curl http://100.100.100.200/latest/meta-data/region-id`aliuid=`curl http://100.100.100.200/latest/meta-data/owner-account-id` helmPackageUrl=&quot;http://logtail-release-$regionId.oss-$regionId.aliyuncs.com/kubernetes/alibaba-cloud-log.tgz&quot;wget $helmPackageUrl -O alibaba-cloud-log.tgzif [ $? != 0 ]; then echo &quot;[FAIL] download alibaba-cloud-log.tgz from $helmPackageUrl failed&quot; exit 1fi project=&quot;k8s-log-&quot;$clusterNameif [ $# -ge 2 ]; then project=$2fi echo [INFO] your k8s is using project : $project helm install alibaba-cloud-log.tgz --name alibaba-log-controller \\ --set ProjectName=$project \\ --set RegionId=$regionId \\ --set InstallParam=$regionId \\ --set MachineGroupId=&quot;k8s-group-&quot;$clusterName \\ --set Endpoint=$regionId&quot;-intranet.log.aliyuncs.com&quot; \\ --set AlibabaCloudUserId=&quot;:&quot;$aliuid \\ --set LogtailImage.Repository=&quot;registry.$regionId.aliyuncs.com/log-service/logtail&quot; \\ --set ControllerImage.Repository=&quot;registry.$regionId.aliyuncs.com/log-service/alibabacloud-log-controller&quot; installRst=$? if [ $installRst -eq 0 ]; then echo &quot;[SUCCESS] install helm package : alibaba-log-controller success.&quot; exit 0else echo &quot;[FAIL] install helm package failed, errno &quot; $installRst exit 0fi 命令执行后，会在kubernetes集群中的每个节点运行一个日志采集的pod：logatail-ds，该pod位于kube-system 安装完成后，可使用以下命令来查看pod状态，若状态全部成功后，则表示安装完成 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[root@izuf6d75c59ll4woscqmh5z ~]# helm status alibaba-log-controllerLAST DEPLOYED: Thu Nov 22 15:09:35 2018NAMESPACE: defaultSTATUS: DEPLOYED RESOURCES:==&gt; v1/ServiceAccountNAME SECRETS AGEalibaba-log-controller 1 6d ==&gt; v1beta1/CustomResourceDefinitionNAME AGEaliyunlogconfigs.log.alibabacloud.com 6d ==&gt; v1beta1/ClusterRolealibaba-log-controller 6d ==&gt; v1beta1/ClusterRoleBindingNAME AGEalibaba-log-controller 6d ==&gt; v1beta1/DaemonSetNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGElogtail-ds 16 16 16 16 16 &lt;none&gt; 6d ==&gt; v1beta1/DeploymentNAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGEalibaba-log-controller 1 1 1 1 6d ==&gt; v1/Pod(related)NAME READY STATUS RESTARTS AGElogtail-ds-2fqs4 1/1 Running 0 6dlogtail-ds-4bz7w 1/1 Running 1 6dlogtail-ds-6vg88 1/1 Running 0 6dlogtail-ds-7tp6v 1/1 Running 0 6dlogtail-ds-9575c 1/1 Running 0 6dlogtail-ds-bgq84 1/1 Running 0 6dlogtail-ds-kdlhr 1/1 Running 0 6dlogtail-ds-lknxw 1/1 Running 0 6dlogtail-ds-pdxfk 1/1 Running 0 6dlogtail-ds-pf4dz 1/1 Running 0 6dlogtail-ds-rzsnw 1/1 Running 0 6dlogtail-ds-sqhbv 1/1 Running 0 6dlogtail-ds-vvtwn 1/1 Running 0 6dlogtail-ds-wwmhg 1/1 Running 0 6dlogtail-ds-xbp4j 1/1 Running 0 6dlogtail-ds-zpld9 1/1 Running 0 6dalibaba-log-controller-85f8fbb498-nzhc8 1/1 Running 0 6d ### 配置日志组件展示 - 在集群内安装好日志组件后，登录阿里云日志服务控制台，就会发现有一个新的project，名称为k8s-log-&#123;集群id&#125; &lt;figure&gt; 创建Logstore 配置Logstore，设置logstore名称、配置日志数据保留时间 数据导入 选择数据类型中选择docker标准输出 数据源配置，这里可以使用默认的 选择数据源 配置好之后等待1-2分钟，日志就会进来了 为了快速查询和过滤，需要配置索引 添加容器名称、命名空间、pod名称作为索引（后续使用livetail需要） 这样就完成了一个k8s集群日志采集和展示的基本流程了 livetail功能使用背景介绍&gt; 在线上运维的场景中，往往需要对日志队列中进入的数据进行实时监控，从最新的日志数据中提取出关键的信息进而快速地分析出异常原因。在传统的运维方式中，如果需要对日志文件进行实时监控，需要到服务器上对日志文件执行命令tail -f，如果实时监控的日志信息不够直观，可以加上grep或者grep -v进行关键词过滤。日志服务在控制台提供了日志数据实时监控的交互功能LiveTail，针对线上日志进行实时监控分析，减轻运维压力。 使用方法 这里选择来源类型为kubernetes，命名空间、pod名称、容器名称为上一步新建的3个索引的内容，过滤关键字的功劳与tail命令后加的grep命令是一样的，用于关键词过滤 点击开启livetail，这时就有实时日志展示出来了 问题与短板阿里云日志服务的livetail功能比起传统的ELK日志展示强大了许多，但是还是存在具体操作需要手动查找、拷贝命名空间、pod名称、容器名称，虽然命名空间和容器名称为固定的，但是pod名称需要查找当前pod的名称，操作起来比较繁琐，功能方面确实不如rancher的实时日志展示，并提供进入终端查看日志文件的功能强大。 同时该功能才测试使用的时候还出现过网络连接错误，闪退等情况，总体来说属于阿里云日志服务的一项新功能，不适合直接给开发运维来使用。 总结由于公司一直使用阿里云的日志服务，总体来说，kubernetes日志采集和展示服务的接入和配置速度很快，如果有需要可以随时拉起并投入使用。 对于现阶段有更方便使用的容器化展示平台的情况下，阿里云livetail方案可以作为一个备选方案，一旦rancher出现问题，可以所以接入和配置使用。 总的来说此方案不如rancher使用灵活和功能强大，不建议推广使用，可作为一个备选方","categories":[],"tags":[{"name":"aliyun","slug":"aliyun","permalink":"https://engining.net/tags/aliyun/"},{"name":"日志组件","slug":"日志组件","permalink":"https://engining.net/tags/%E6%97%A5%E5%BF%97%E7%BB%84%E4%BB%B6/"},{"name":"livetail","slug":"livetail","permalink":"https://engining.net/tags/livetail/"},{"name":"k8s","slug":"k8s","permalink":"https://engining.net/tags/k8s/"}],"author":"凯京.郭旭东"},{"title":"understanding spring boot","slug":"archive/2018-02-04-understanding spring boot","date":"2018-02-03T16:00:00.000Z","updated":"2021-09-27T03:01:02.448Z","comments":true,"path":"2018/02/04/archive/2018-02-04-understanding spring boot/","link":"","permalink":"https://engining.net/2018/02/04/archive/2018-02-04-understanding%20spring%20boot/","excerpt":"","text":"Spring boot is an opinionated library that allows to create executable Spring applications with a convention over configuration approach. The magic behind this framework lies in the @EnableAutoConfiguration annotation, which will automatically load all the beans the application requires depending on what Spring Boot finds in the classpath. The @Enable* annotationsThe @Enable… annotations are not new, they were first introduced in Spring 3 when the idea of replacing the XML files with java annotated classes is born. A lot of Spring users already know @EnableTransactionManagement, which will enable declarative transaction management, @EnableWebMvc, which enables Spring MVC, or @EnableScheduling, which will initialize a scheduler. These annotations are in fact a simple configuration import with the @Import annotation. 12345678910111213@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documented@Import(&#123; EnableAutoConfigurationImportSelector.class, AutoConfigurationPackages.Registrar.class &#125;)public @interface EnableAutoConfiguration &#123; /** * Exclude specific auto-configuration classes such that they will never be applied. */ Class&lt;?&gt;[] exclude() default &#123;&#125;;&#125; The EnableAutoConfigurationImportSelector uses SpringFactoriesLoader#loadFactoryNames of Spring core. SpringFactoriesLoader will look for jars containing a file with the path META-INF&#x2F;spring.factories. When it finds such a file, the SpringFactoriesLoader will look for the property named after our configuration file. In our case, org.springframework.boot.autoconfigure.EnableAutoConfiguration. Let’s take a look at the spring-boot-autoconfigure jar, which indeed contains a spring.factories file copied below: Initializers12org.springframework.context.ApplicationContextInitializer=\\org.springframework.boot.autoconfigure.logging.AutoConfigurationReportLoggingInitializer Auto Configure1234567891011121314151617181920212223242526272829org.springframework.boot.autoconfigure.EnableAutoConfiguration=\\org.springframework.boot.autoconfigure.aop.AopAutoConfiguration,\\org.springframework.boot.autoconfigure.amqp.RabbitAutoConfiguration,\\org.springframework.boot.autoconfigure.MessageSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.PropertyPlaceholderAutoConfiguration,\\org.springframework.boot.autoconfigure.batch.BatchAutoConfiguration,\\org.springframework.boot.autoconfigure.data.JpaRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.data.MongoRepositoriesAutoConfiguration,\\org.springframework.boot.autoconfigure.redis.RedisAutoConfiguration,\\org.springframework.boot.autoconfigure.jdbc.DataSourceAutoConfiguration,\\org.springframework.boot.autoconfigure.jdbc.DataSourceTransactionManagerAutoConfiguration,\\org.springframework.boot.autoconfigure.jms.JmsTemplateAutoConfiguration,\\org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration,\\org.springframework.boot.autoconfigure.mobile.DeviceResolverAutoConfiguration,\\org.springframework.boot.autoconfigure.mongo.MongoAutoConfiguration,\\org.springframework.boot.autoconfigure.mongo.MongoTemplateAutoConfiguration,\\org.springframework.boot.autoconfigure.orm.jpa.HibernateJpaAutoConfiguration,\\org.springframework.boot.autoconfigure.reactor.ReactorAutoConfiguration,\\org.springframework.boot.autoconfigure.security.SecurityAutoConfiguration,\\org.springframework.boot.autoconfigure.security.FallbackWebSecurityAutoConfiguration,\\org.springframework.boot.autoconfigure.thymeleaf.ThymeleafAutoConfiguration,\\org.springframework.boot.autoconfigure.web.EmbeddedServletContainerAutoConfiguration,\\org.springframework.boot.autoconfigure.web.DispatcherServletAutoConfiguration,\\org.springframework.boot.autoconfigure.web.ServerPropertiesAutoConfiguration,\\org.springframework.boot.autoconfigure.web.MultipartAutoConfiguration,\\org.springframework.boot.autoconfigure.web.HttpMessageConvertersAutoConfiguration,\\org.springframework.boot.autoconfigure.web.WebMvcAutoConfiguration,\\org.springframework.boot.autoconfigure.websocket.WebSocketAutoConfiguration In this file, we can see a list of the Spring Boot auto-configurations. Let’s take a closer look at one of those configurations, MongoAutoConfiguration for instance: 1234567891011121314151617181920212223242526@Configuration@ConditionalOnClass(Mongo.class)@EnableConfigurationProperties(MongoProperties.class)public class MongoAutoConfiguration &#123; @Autowired private MongoProperties properties; private Mongo mongo; @PreDestroy public void close() throws UnknownHostException &#123; if (this.mongo != null) &#123; this.mongo.close(); &#125; &#125; @Bean @ConditionalOnMissingBean public Mongo mongo() throws UnknownHostException &#123; this.mongo = this.properties.createMongoClient(); return this.mongo; &#125;&#125; This simple Spring configuration class declares typical beans needed to use mongoDb. This classes, like a lot of others in Spring Boot relies heavily on Spring annotations: @ConditionOnClass activates a configuration only if one or several classes are present on the classpath@EnableConfigurationProperties automatically maps a POJO to a set of properties in the Spring Boot configuration file (by default application.properties)@ConditionalOnMissingBean enables a bean definition only if the bean wasn’t previously definedYou can also refine the order in which those configuration classes load with @AutoConfigureBefore et @AutoConfigureAfter. Properties MappingLet’s look at MongoProperties, which is a classic example of Spring Boot properties mapping: 12345678910@ConfigurationProperties(prefix = &quot;spring.data.mongodb&quot;)public class MongoProperties &#123; private String host; private int port = DBPort.PORT; private String uri = &quot;mongodb://localhost/test&quot;; private String database; // ... getters/ setters omitted&#125; The @ConfigurationProperties will associate every properties with a particular prefix to the POJO. For instance, the property spring.data.mongodb.port will be mapped to the port attribute of this class. If you’re a Spring Boot user, I strongly encourage you to use those capabilities to remove the boiler plate code associated with configuration properties. The @Conditional annotationsThe power of Spring Boot lies in one of Spring 4 new features: the @Conditional annotations, which will enable some configuration only if a specific condition is met. A sneak peek in the org.springframework.boot.autoconfigure.condition package in Spring Boot will give us an overview of what we can do with those annotations: 12345678910111213141516171819202122@ConditionalOnBean@ConditionalOnClass@ConditionalOnExpression@ConditionalOnMissingBean@ConditionalOnMissingClass@ConditionalOnNotWebApplication@ConditionalOnResource@ConditionalOnWebApplicationLet’s take a closer look at @ConditionalOnExpression, which allows you to write a condition in the Spring Expression language.@Conditional(OnExpressionCondition.class)@Retention(RetentionPolicy.RUNTIME)@Target(&#123; ElementType.TYPE, ElementType.METHOD &#125;)public @interface ConditionalOnExpression &#123; /** * The SpEL expression to evaluate. Expression should return &#123;@code true&#125; if the * condition passes or &#123;@code false&#125; if it fails. */ String value() default &quot;true&quot;;&#125; In this class, we indeed make use of the @Conditional annotation. The condition is defined in the OnExpressionCondition class: 123456789101112131415public class OnExpressionCondition extends SpringBootCondition &#123; @Override public ConditionOutcome getMatchOutcome(ConditionContext context, AnnotatedTypeMetadata metadata) &#123; // ... // we first get a handle on the EL context via the ConditionContext boolean result = (Boolean) resolver.evaluate(expression, expressionContext); // ... // here we create a message the user will see when debugging return new ConditionOutcome(result, message.toString()); &#125;&#125; In the end, the @Conditional are resolved to simple booleans, via the ConditionOutcome.isMatch method. The ApplicationContextInitializersThe second possibility that the spring.factories file offers, is to define application initializers. They allow us to manipulate Spring’s applicationContext before the application is loaded. In particular, they can create listeners on the context thanks to the ConfigurableApplicationContext#addApplicationListener method. Spring Boot does that in the AutoConfigurationReportLoggingInitializer which listens to system events, like context refresh or the application’s failure to start. This will help create the auto-configuration report when you start your application in debug mode. You can start your application in debug mode with either the -Ddebug flag or add the property debug&#x3D;true to application.properties. Debug Spring Boot Auto-ConfigurationThe documentation gives us some advice to understand what happened during the auto-configuration. When launched in debug mode, Spring Boot will generate a report that looks like this one: 12345678910111213141516171819202122232425262728293031323334353637383940414243Positive matches:----------------- MessageSourceAutoConfiguration - @ConditionalOnMissingBean (types: org.springframework.context.MessageSource; SearchStrategy: all) found no beans (OnBeanCondition) JmxAutoConfiguration - @ConditionalOnClass classes found: org.springframework.jmx.export.MBeanExporter (OnClassCondition) - SpEL expression on org.springframework.boot.autoconfigure.jmx.JmxAutoConfiguration: $&#123;spring.jmx.enabled:true&#125; (OnExpressionCondition) - @ConditionalOnMissingBean (types: org.springframework.jmx.export.MBeanExporter; SearchStrategy: all) found no beans (OnBeanCondition) DispatcherServletAutoConfiguration - found web application StandardServletEnvironment (OnWebApplicationCondition) - @ConditionalOnClass classes found: org.springframework.web.servlet.DispatcherServlet (OnClassCondition)Negative matches:----------------- DataSourceAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.jdbc.datasource.embedded.EmbeddedDatabaseType (OnClassCondition) DataSourceTransactionManagerAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.jdbc.core.JdbcTemplate,org.springframework.transaction.PlatformTransactionManager (OnClassCondition) MongoAutoConfiguration - required @ConditionalOnClass classes not found: com.mongodb.Mongo (OnClassCondition) FallbackWebSecurityAutoConfiguration - SpEL expression on org.springframework.boot.autoconfigure.security.FallbackWebSecurityAutoConfiguration: !$&#123;security.basic.enabled:true&#125; (OnExpressionCondition) SecurityAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.security.authentication.AuthenticationManager (OnClassCondition) EmbeddedServletContainerAutoConfiguration.EmbeddedJetty - required @ConditionalOnClass classes not found: org.eclipse.jetty.server.Server,org.eclipse.jetty.util.Loader (OnClassCondition) WebMvcAutoConfiguration.WebMvcAutoConfigurationAdapter#localeResolver - @ConditionalOnMissingBean (types: org.springframework.web.servlet.LocaleResolver; SearchStrategy: all) found no beans (OnBeanCondition) - SpEL expression: &#x27;$&#123;spring.mvc.locale:&#125;&#x27; != &#x27;&#x27; (OnExpressionCondition) WebSocketAutoConfiguration - required @ConditionalOnClass classes not found: org.springframework.web.socket.WebSocketHandler,org.apache.tomcat.websocket.server.WsSci (OnClassCondition) For each auto-configuration, we can see why it was initiated or why it failed. ConclusionSpring Boot’s approach leverages the possibilities of Spring 4 and allows to create an auto-configured executable jar. Don’t forget that, as the documentation states, you can gradually replace the auto-configuration by declaring your own beans. What I love about Spring Boot is that it allows you to prototype an application very quickly but also to learn with its source. Auto-configurations are neat pieces of code that can teach you a thing or two about Spring. copyright: https://geowarin.github.io/understanding-spring-boot.html","categories":[],"tags":[{"name":"springboot @Enable*","slug":"springboot-Enable","permalink":"https://engining.net/tags/springboot-Enable/"}]},{"title":"分布式缓存学习心得","slug":"archive/2018-01-14-分布式缓存","date":"2018-01-13T16:00:00.000Z","updated":"2021-09-27T03:01:02.447Z","comments":true,"path":"2018/01/14/archive/2018-01-14-分布式缓存/","link":"","permalink":"https://engining.net/2018/01/14/archive/2018-01-14-%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/","excerpt":"","text":"缓存为王 音乐是时间的艺术,缓存是软件时间的艺术 1. 客户端缓存2. 页面缓存2.1 浏览器缓存 2.2 APP上的缓存 3. 网络中的缓存3.1 web代理缓存 squid 3.2 边缘缓存 varinish CDN cloud front in aws china cache 4. 服务端的缓存4.1 数据库缓存 4.2 平台级缓存 一级缓存 4.2 应用级缓存 REDIS 集群 中间件twemproxy: https://github.com/twitter/twemproxy redis4.0、codis、阿里云redis 3种redis集群对比分析 https://yq.aliyun.com/articles/68593?utm_campaign=wenzhang&amp;utm_medium=article&amp;utm_source=QQ-qun&amp;utm_content=m_10099 多级缓存 分布式系统理论 未完继续","categories":[],"tags":[{"name":"分布式缓存","slug":"分布式缓存","permalink":"https://engining.net/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"Redis","slug":"Redis","permalink":"https://engining.net/tags/Redis/"},{"name":"memcache","slug":"memcache","permalink":"https://engining.net/tags/memcache/"},{"name":"guavacache","slug":"guavacache","permalink":"https://engining.net/tags/guavacache/"},{"name":"ehcache","slug":"ehcache","permalink":"https://engining.net/tags/ehcache/"},{"name":"tair","slug":"tair","permalink":"https://engining.net/tags/tair/"}]},{"title":"DevOps 文章分享","slug":"archive/2018-01-13-DevOps知识分享2篇","date":"2018-01-12T16:00:00.000Z","updated":"2021-09-27T03:01:02.447Z","comments":true,"path":"2018/01/13/archive/2018-01-13-DevOps知识分享2篇/","link":"","permalink":"https://engining.net/2018/01/13/archive/2018-01-13-DevOps%E7%9F%A5%E8%AF%86%E5%88%86%E4%BA%AB2%E7%AF%87/","excerpt":"","text":"DevOps 文章分享, 抽出时间再整理 大象翩翩起舞！国外大型银行 DevOps 转型干货总结 炫酷实用的 DevOps 仪表盘，你值得拥有的交付流水线信息整合工具","categories":[],"tags":[{"name":"DevOps","slug":"DevOps","permalink":"https://engining.net/tags/DevOps/"},{"name":"大盘","slug":"大盘","permalink":"https://engining.net/tags/%E5%A4%A7%E7%9B%98/"},{"name":"持续交付","slug":"持续交付","permalink":"https://engining.net/tags/%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98/"}],"author":"DevOps时代.张乐"},{"title":"aspectJ 编译时织入遇到的3个问题","slug":"archive/2018-01-13-aspectj编译时织入遇到的坑","date":"2018-01-12T16:00:00.000Z","updated":"2021-09-27T03:01:02.447Z","comments":true,"path":"2018/01/13/archive/2018-01-13-aspectj编译时织入遇到的坑/","link":"","permalink":"https://engining.net/2018/01/13/archive/2018-01-13-aspectj%E7%BC%96%E8%AF%91%E6%97%B6%E7%BB%87%E5%85%A5%E9%81%87%E5%88%B0%E7%9A%84%E5%9D%91/","excerpt":"","text":"aspectJ 编译时织入 结合spring动态代理时遇到的3个问题： 第1个问题 自定义注解 拦截器 编译后的class： 当调用 bizServcie.loanAmount时进入了2次，看编译后的类确实是调用者method和加@Voucher的metod都被编译植入了 问题出在： 12@Around(&quot;@annotation(net.keking.lbt.business.vou.annotation.Voucher)&quot;) 这个是 springaop语法，并不是aspectJ语法正确的pointcut 是： 1@Around(&quot;execution(@net.keking.lbt.business.vou.annotation.Voucher * *(..))&quot;) 目前编译后的效果： 第2个问题@Aspect 拦截器无法注入其它servcie的问题，因为@aspect 的注解是ACJ编译器编译的，没有纳入spring的管理需要增加： 1&lt;bean id=&quot;voucherAspectJ&quot; class=&quot;net.keking.lbt.business.vou.VoucherAopHandler&quot; factory-method=&quot;aspectOf&quot;/&gt; 从而纳入spring上下文容器 第3个问题如果自定义注解在project中单独的module ,如lbt-aspects， 其它moudule如何引用此自定义注解POM中需要修改： 123456789101112&lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;aspectj-maven-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;aspectLibraries&gt; &lt;aspectLibrary&gt; &lt;groupId&gt;net.keking.lbt&lt;/groupId&gt; &lt;artifactId&gt;lbt-aspects&lt;/artifactId&gt; &lt;/aspectLibrary&gt; &lt;/aspectLibraries&gt; &lt;/configuration&gt;&lt;/plugin&gt;","categories":[],"tags":[{"name":"Aspectj","slug":"Aspectj","permalink":"https://engining.net/tags/Aspectj/"},{"name":"aop","slug":"aop","permalink":"https://engining.net/tags/aop/"},{"name":"spring","slug":"spring","permalink":"https://engining.net/tags/spring/"}],"author":"tangshd"},{"title":"开源许可协议了解这些就够了","slug":"archive/2018-01-02-开源协议","date":"2018-01-01T16:00:00.000Z","updated":"2021-09-27T03:01:02.447Z","comments":true,"path":"2018/01/02/archive/2018-01-02-开源协议/","link":"","permalink":"https://engining.net/2018/01/02/archive/2018-01-02-%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE/","excerpt":"","text":"我们开发中经常会用到一些开源框架, 每个框架一般都有自己的开源协议. 本文介绍了6种常用的开源协议, 让你用得明白放心~ 一、如何选择开源协议？现今存在的开源协议很多，可以在（http://www.opensource.org/licenses/alphabetical ）详细查看。我们最常用到的开源协议有6种，这些都是OSI 批准的协议，也是绝大多数公司会用到的协议。 GPL LGPL BSD Apache Mozilla MIT 下图是网上流行的一张协议分析图，能让我们一目了然的去判断该选择何种开源协议（原著：乌克兰程序员Paul Bagwell，翻译：阮一峰）。 看完这张图，是不是心里有底了呢？稍微总结一下: 如果你在商业公司上班，最好不要使用GPL协议的开源软件，因为它具有“传染性”，并且强制开源，只要引入的某个模块是GPL的，它会一直扩展到最上层知道整个项目都强制GPL开源。 用BSD、Apache或者MIT的开源项目则一般不会有问题，只需要开源项目本身的安全性或者健壮性等其他需求满足公司要求即可。 另外要说明一下的是，有协议和没声明协议的裸代码是有非常重要区别的，一般作品当中没声明协议的默认为Copy right的，也就是版权保留。此种情况表明他人没有任何授权，不得复制分发修改使用等等。所以如果要开源自己的代码，最好也是选择这些著名的开源协议。 二、6中常用开源协议介绍最后，针对上面提到的六种开源许可协议做一个简单资料整理和介绍，供大家查阅。 1. BSD(original BSD license 、 FreeBSD license 、 Original BSD license) BSD 开源协议是一个给于使用者很大自由的协议。基本上使用者可以” 为所欲为”, 可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件再发布。但” 为所欲为” 的前提当你发布使用了BSD 协议的代码，或则以BSD 协议代码为基础做二次开发自己的产品时，需要满足三个条件： 如果再发布的产品中包含源代码，则在源代码中必须带有原来代码中的BSD 协议。如果再发布的只是二进制类库&#x2F; 软件，则需要在类库&#x2F; 软件的文档和版权声明中包含原来代码中的BSD 协议。不可以用开源代码的作者&#x2F; 机构名字和原来产品的名字做市场推广。BSD 代码鼓励代码共享，但需要尊重代码作者的著作权。BSD 由于允许使用者修改和重新发布代码，也允许使用或在BSD 代码上开发商业软件发布和销售，因此是对 商业集成很友好的协议。而很多的公司企业在选用开源产品的时候都首选BSD 协议，因为可以完全控制这些第三方的代码，在必要的时候可以修改或者二次开发。 2. Apache Licence 2.0(Apache License, Version 2.0 、Apache License, Version 1.1 、Apache License, Version 1.0) Apache Licence 是著名的非盈利开源组织Apache 采用的协议。该协议和BSD 类似，同样鼓励代码共享和尊重原作者的著作权，同样允许代码修改，再发布（作为开源或商业软件）。 需要满足的条件也和BSD 类似：需要给代码的用户一份Apache Licence如果你修改了代码，需要再被修改的文件中说明。在延伸的代码中（修改和有源代码衍生的代码中）需要带有原来代码中的协议，商标，专利声明和其他原来作者规定需要包含的说明。如果再发布的产品中包含一个Notice 文件，则在Notice 文件中需要带有Apache Licence 。你可以在Notice 中增加自己的许可，但不可以表现为对Apache Licence 构成更改。Apache Licence 也是对商业应用友好的许可。使用者也可以在需要的时候修改代码来满足需要并作为开源或商业产品发布&#x2F; 销售。 3.GPL(GNU General Public License) 我们很熟悉的Linux 就是采用了GPL 。GPL 协议和BSD, Apache Licence 等鼓励代码重用的许可很不一样。GPL不允许修改后和衍生的代码做为闭源的商业软件发布和销售。这也就是为什么我们能用免费的各种linux ，包括商业公司的linux 和linux 上各种各样的由个人，组织，以及商业软件公司开发的免费软件了。 GPL 协议的主要内容是只要在一个软件中使用(” 使用” 指类库引用，修改后的代码或者衍生代码)GPL 协议的产品，则该软件产品必须也采用GPL 协议，既必须也是开源和免费。这就是所谓的” 传染性”。GPL 协议的产品作为一个单独的产品使用没有任何问题，还可以享受免费的优势。由于GPL 严格要求使用了GPL 类库的软件产品必须使用GPL 协议，对于使用GPL 协议的开源代码，商业软件或者对代码有保密要求的部门就不适合集成&#x2F; 采用作为类库和二次开发的基础。 4.LGPL(GNU Lesser General Public License) LGPL 是GPL 的一个为主要为类库使用设计的开源协议。和GPL 要求任何使用&#x2F; 修改&#x2F; 衍生之GPL 类库的的软件必须采用GPL 协议不同。LGPL 允许商业软件通过类库引用(link) 方式使用LGPL 类库而不需要开源商业软件的代码。这使得采用LGPL 协议的开源代码可以被商业软件作为类库引用并 发布和销售。 但是如果修改LGPL 协议的代码或者衍生，则所有修改的代码，涉及修改部分的额外代码和衍生的代码都必须采用LGPL 协议。因此LGPL 协议的开源 代码很适合作为第三方类库被商业软件引用，但不适合希望以LGPL 协议代码为基础，通过修改和衍生的方式做二次开发的商业软件采用。 5.MIT （The MIT License） MIT 是和BSD 一样宽范的许可协议, 作者只想保留版权, 而无任何其他了限制. 也就是说, 你必须在你的发行版里包含原许可协议的声明, 无论你是以二进制发布的还是以源代码发布的. 6. MPL(Mozilla Public License) MPL协议允许免费重发布、免费修改，但要求修改后的代码版权归软件的发起者 。这种授权维护了商业软件的利益，它要求基于这种软件的修改无偿贡献版权给该软件。 作者：菜刀文链接：https://www.jianshu.com/p/a57c13631d5e來源：简书著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。","categories":[],"tags":[{"name":"开源协议","slug":"开源协议","permalink":"https://engining.net/tags/%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE/"}]},{"title":"第一个开源项目","slug":"archive/2017-12-19-公司开源项目","date":"2017-12-18T16:00:00.000Z","updated":"2021-09-27T03:01:02.447Z","comments":true,"path":"2017/12/19/archive/2017-12-19-公司开源项目/","link":"","permalink":"https://engining.net/2017/12/19/archive/2017-12-19-%E5%85%AC%E5%8F%B8%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE/","excerpt":"","text":"项目介绍解决了文件文档在线预览的问题，对标业内付费产品有【永中office】【office365】【idocv】等，该项目基本支持主流办公文档的在线预览，如doc,docx,Excel,pdf,txt,zip,rar,图片等等 项目地址 码云：https://gitee.com/kekingcn/file-online-preview github：https://github.com/kekingcn/file-online-preview 公司简介上海凯京信达科技集团有限公司（“凯京集团”），2015年成立于上海陆家嘴，专注为中小微企业和个人提供各类场景下的信贷服务，产品包括商业保理业务、融资租赁业务、供应链金融、大数据征信等。 公司2015年获得知名投资机构数千万元人民币天使轮投资，2016年2月获得红杉资本领投的1亿元人民币A轮投资，2016年12月获得由中航信托领投、红杉资本、复朴资本跟投的2亿人民币B轮融资。 我们的使命：数据重构物流 我们的愿景：最具价值的数据科技企业 公司网址凯京集团","categories":[],"tags":[{"name":"开源","slug":"开源","permalink":"https://engining.net/tags/%E5%BC%80%E6%BA%90/"},{"name":"在线预览","slug":"在线预览","permalink":"https://engining.net/tags/%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88/"},{"name":"office","slug":"office","permalink":"https://engining.net/tags/office/"},{"name":"zip","slug":"zip","permalink":"https://engining.net/tags/zip/"},{"name":"pdf","slug":"pdf","permalink":"https://engining.net/tags/pdf/"}],"author":"凯京"},{"title":"日志收集实践参考","slug":"archive/2017-12-15-日志收集实践参考","date":"2017-12-14T16:00:00.000Z","updated":"2021-09-27T03:01:02.446Z","comments":true,"path":"2017/12/15/archive/2017-12-15-日志收集实践参考/","link":"","permalink":"https://engining.net/2017/12/15/archive/2017-12-15-%E6%97%A5%E5%BF%97%E6%94%B6%E9%9B%86%E5%AE%9E%E8%B7%B5%E5%8F%82%E8%80%83/","excerpt":"","text":"一. 日志规约 记日志的目的就是为了快速定位问题 参见阿里手册 二. 本地测试环境准备 今天主要讲的是 logs -&gt; logstash -&gt; kafka ，本地使用docker模拟 (一) 启动zookeeper 拉取zookeeper docker镜像 1docker pull wurstmeister/zookeeper 启动zookeeper 1docker run -d --name zookeeper -p 2181 -t wurstmeister/zookeeper （二） 启动Kafka，查看接收消息 拉取kafka docker镜像 1docker pull wurstmeister/kafka 启动kafka 12docker run --name kafka -e HOST_IP=localhost -e KAFKA_ADVERTISED_PORT=9092 -e KAFKA_BROKER_ID=1 -e ZK=zk -p 9092 --link zookeeper:zk -t wurstmeister/kafkadocker ps 创建一个topic 123docker exec -it $&#123;CONTAINER ID&#125; /bin/bashcd opt/kafka_2.12-1.0.0/bin/kafka-topics.sh --create --zookeeper zookeeper:2181 --replication-factor 1 --partitions 1 --topic biz-logs 新打开一个shell起一个消费者 12cd opt/kafka_2.12-1.0.0/bin/kafka-console-consumer.sh --zookeeper zookeeper:2181 --topic biz-logs --from-beginning 新打开一个shell起一个生产者测试下 12cd opt/kafka_2.12-1.0.0/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic biz-logs 在生产者输入，显示在消费者的输出中，表明启动正常了 (三) 启动logstash 拉取logstash docker 镜像 1docker pull logstash 创建udp接收，输出到kafka的配置文件udp-kafka.conf 123456789101112131415161718192021222324252627input &#123; gelf &#123; port =&gt; 12201 type =&gt; docker codec =&gt; json &#125; udp &#123; type =&gt; docker port =&gt; 10800 codec =&gt; json &#125; tcp &#123; type =&gt; docker port =&gt; 10800 codec =&gt; json &#125;&#125;filter &#123;&#125;output &#123; stdout &#123; codec =&gt; rubydebug &#125; kafka &#123; topic_id =&gt; &quot;biz-logs&quot; bootstrap_servers =&gt; &quot;ka:9092&quot; codec =&gt; &quot;json&quot; &#125;&#125; 启动logstash（执行命令需要在上面配置文件同一个目录下，windows系统把$PWD改成配置文件所在目录的绝对路径） 1docker run -it --rm -v &quot;$PWD&quot;:/config-dir -p 10800:10800/udp -p 10800:10800/tcp -p 12201:12201/udp --link kafka:ka logstash -f /config-dir/udp-kafka.conf 三. logback 日志输出前置 配置appName,在pom文件中增加 123&lt;properties&gt; &lt;appName&gt;authority&lt;/appName&gt;&lt;/properties&gt; 在.gitignore文件中添加忽略 1logs/ 日志数据 默认已有数据 Field Description @timestamp Time of the log event. (yyyy-MM-dd’T’HH:mm:ss.SSSZZ) See customizing timezone. @version Logstash format version (e.g. 1) See customizing version. message Formatted log message of the event logger_name Name of the logger that logged the event thread_name Name of the thread that logged the event level String name of the level of the event level_value Integer value of the level of the event stack_trace (Only if a throwable was logged) The stacktrace of the throwable. Stackframes are separated by line endings. tags (Only if tags are found) The names of any markers not explicitly handled. (e.g. markers from MarkerFactory.getMarker will be included as tags, but the markers from Markers will not.) 打开includeCallerData Field Description caller_class_name Fully qualified class name of the class that logged the event caller_method_name Name of the method that logged the event caller_file_name Name of the file that logged the event caller_line_number Line number of the file where the event was logged （一）配置日志本地输出 配置pom文件，确定输出路径 1234567&lt;!-- 线上环境 --&gt;&lt;!-- 日志存放地址 --&gt;&lt;logging.path&gt;/data/logs/$&#123;appName&#125;&lt;/logging.path&gt;&lt;!-- 本地环境 --&gt;&lt;!-- 日志存放地址 --&gt;&lt;logging.path&gt;$&#123;basedir&#125;/logs&lt;/logging.path&gt; 配置logback.xml，添加console输出 123456&lt;appender name=&quot;consoleRolling&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt;&lt;/appender&gt; 配置logback.xml，添加本地日志输出 123456789101112131415161718192021222324252627282930&lt;appender name=&quot;dailyRollingFile&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;File&gt;$&#123;logging.path&#125;/app.log&lt;/File&gt; &lt;!-- rollingPolicy下面两种配置都是可以的，上面那种更简洁一点 --&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt; &lt;!-- daily rollover --&gt; &lt;fileNamePattern&gt;$&#123;logging.path&#125;/app.%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;/rollingPolicy&gt; &lt;!--&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;--&gt; &lt;!--&lt;fileNamePattern&gt;$&#123;logging.path&#125;/app-%d&#123;yyyyMMdd&#125;.%i.log&lt;/fileNamePattern&gt;--&gt; &lt;!--&lt;timeBasedFileNamingAndTriggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP&quot;&gt;--&gt; &lt;!--&lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt;--&gt; &lt;!--&lt;/timeBasedFileNamingAndTriggeringPolicy&gt;--&gt; &lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt; &lt;!--&lt;/rollingPolicy&gt;--&gt; &lt;encoder&gt; &lt;!-- %d&#123;HH:mm:ss.SSS&#125;——日志输出时间，因为每个文件都是按天存放的，所以没必要再输出年月日 %X&#123;traceId&#125;/%X&#123;spanId&#125;——预留 $&#123;appName&#125;——pom文件中定义的应用名 %thread——输出日志的进程名字，这在Web应用以及异步任务处理中很有用 %-5level——日志级别，并且使用5个字符靠左对齐 %logger&#123;36&#125;——日志输出者的名字 %msg——日志消息 %n——平台的换行符 --&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%X&#123;traceId&#125;/%X&#123;spanId&#125;] $&#123;appName&#125; [%thread] %-5level %logger&#123;35&#125; - %msg %n&lt;/pattern&gt; &lt;/encoder&gt;&lt;/appender&gt; 启动项目，看到日志输出 日志文件输出 123456789101115:36:58.036 [/] authority [main] INFO org.hibernate.Version - HHH000412: Hibernate Core &#123;5.0.12.Final&#125;15:36:58.038 [/] authority [main] INFO org.hibernate.cfg.Environment - HHH000206: hibernate.properties not found15:36:58.039 [/] authority [main] INFO org.hibernate.cfg.Environment - HHH000021: Bytecode provider name : javassist15:36:58.090 [/] authority [main] INFO o.h.annotations.common.Version - HCANN000001: Hibernate Commons Annotations &#123;5.0.1.Final&#125;15:36:58.251 [/] authority [main] INFO c.a.druid.pool.DruidDataSource - &#123;dataSource-1&#125; inited15:36:58.895 [/] authority [main] INFO org.hibernate.dialect.Dialect - HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect15:36:59.373 [/] authority [main] INFO o.h.tool.hbm2ddl.SchemaUpdate - HHH000228: Running hbm2ddl schema update15:37:21.387 [/] authority [main] INFO o.s.o.j.LocalContainerEntityManagerFactoryBean - Initialized JPA EntityManagerFactory for persistence unit &#x27;default&#x27;15:37:21.540 [/] authority [main] INFO org.redisson.Version - Redisson 2.3.015:37:21.661 [/] authority [nioEventLoopGroup-2-7] INFO o.r.c.p.SinglePubSubConnectionPool - 1 connections initialized for /192.168.1.204:637915:37:21.662 [/] authority [nioEventLoopGroup-2-2] INFO o.r.c.pool.MasterConnectionPool - 5 connections initialized for /192.168.1.204:6379 console输出 12345678910112017-11-21 15:36:58.036 INFO 1899 --- [ main] org.hibernate.Version : HHH000412: Hibernate Core &#123;5.0.12.Final&#125;2017-11-21 15:36:58.038 INFO 1899 --- [ main] org.hibernate.cfg.Environment : HHH000206: hibernate.properties not found2017-11-21 15:36:58.039 INFO 1899 --- [ main] org.hibernate.cfg.Environment : HHH000021: Bytecode provider name : javassist2017-11-21 15:36:58.090 INFO 1899 --- [ main] o.hibernate.annotations.common.Version : HCANN000001: Hibernate Commons Annotations &#123;5.0.1.Final&#125;2017-11-21 15:36:58.251 INFO 1899 --- [ main] com.alibaba.druid.pool.DruidDataSource : &#123;dataSource-1&#125; inited2017-11-21 15:36:58.895 INFO 1899 --- [ main] org.hibernate.dialect.Dialect : HHH000400: Using dialect: org.hibernate.dialect.MySQL5Dialect2017-11-21 15:36:59.373 INFO 1899 --- [ main] org.hibernate.tool.hbm2ddl.SchemaUpdate : HHH000228: Running hbm2ddl schema update2017-11-21 15:37:21.387 INFO 1899 --- [ main] j.LocalContainerEntityManagerFactoryBean : Initialized JPA EntityManagerFactory for persistence unit &#x27;default&#x27;2017-11-21 15:37:21.540 INFO 1899 --- [ main] org.redisson.Version : Redisson 2.3.02017-11-21 15:37:21.661 INFO 1899 --- [ntLoopGroup-2-7] o.r.c.pool.SinglePubSubConnectionPool : 1 connections initialized for /192.168.1.204:63792017-11-21 15:37:21.662 INFO 1899 --- [ntLoopGroup-2-2] o.r.c.pool.MasterConnectionPool : 5 connections initialized for /192.168.1.204:6379 （二）使用udp输出到logstash mdc默认添加到json输出的日志里 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt;&lt;/dependency&gt; 配置logback.xml，加上以下内容（通过udp协议发送到logstash，其他的方式参考 https://github.com/logstash/logstash-logback-encoder/tree/logstash-logback-encoder-4.11 ） 12345&lt;appender name=&quot;stash&quot; class=&quot;net.logstash.logback.appender.LogstashSocketAppender&quot;&gt; &lt;port&gt;10800&lt;/port&gt;/ &lt;includeCallerData&gt;true&lt;/includeCallerData&gt; &lt;customFields&gt;&#123;&quot;app_name&quot;:&quot;authority&quot;&#125;&lt;/customFields&gt;&lt;/appender&gt; 启动项目，看到日志输出 123456789101112131415161718192021222324252627282930313233&#123; &quot;caller_file_name&quot; =&gt; &quot;DruidDataSource.java&quot;, &quot;level&quot; =&gt; &quot;INFO&quot;, &quot;caller_line_number&quot; =&gt; 1658, &quot;message&quot; =&gt; &quot;&#123;dataSource-1&#125; closed&quot;, &quot;type&quot; =&gt; &quot;docker&quot;, &quot;caller_method_name&quot; =&gt; &quot;close&quot;, &quot;@timestamp&quot; =&gt; 2017-11-16T08:39:49.543Z, &quot;app_name&quot; =&gt; &quot;authority&quot;, &quot;caller_class_name&quot; =&gt; &quot;com.alibaba.druid.pool.DruidDataSource&quot;, &quot;thread_name&quot; =&gt; &quot;Thread-69&quot;, &quot;level_value&quot; =&gt; 20000, &quot;@version&quot; =&gt; 1, &quot;host&quot; =&gt; &quot;172.17.0.1&quot;, &quot;logger_name&quot; =&gt; &quot;com.alibaba.druid.pool.DruidDataSource&quot;&#125;&#123; &quot;caller_file_name&quot; =&gt; &quot;SpringApplication.java&quot;, &quot;level&quot; =&gt; &quot;ERROR&quot;, &quot;caller_line_number&quot; =&gt; 771, &quot;message&quot; =&gt; &quot;Application startup failed&quot;, &quot;type&quot; =&gt; &quot;docker&quot;, &quot;caller_method_name&quot; =&gt; &quot;reportFailure&quot;, &quot;@timestamp&quot; =&gt; 2017-11-16T08:39:47.303Z, &quot;app_name&quot; =&gt; &quot;authority&quot;, &quot;caller_class_name&quot; =&gt; &quot;org.springframework.boot.SpringApplication&quot;, &quot;thread_name&quot; =&gt; &quot;main&quot;, &quot;level_value&quot; =&gt; 40000, &quot;@version&quot; =&gt; 1, &quot;host&quot; =&gt; &quot;172.17.0.1&quot;, &quot;logger_name&quot; =&gt; &quot;org.springframework.boot.SpringApplication&quot;, &quot;stack_trace&quot; =&gt; &quot;java.lang.IllegalStateException: Failed to execute CommandLineRunner\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:735)\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:716)\\n\\tat org.springframework.boot.SpringApplication.afterRefresh(SpringApplication.java:703)\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:304)\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1118)\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1107)\\n\\tat com.yudianbank.project.YdjfBootAuthorityApplication.main(YdjfBootAuthorityApplication.java:20)\\nCaused by: org.redisson.client.RedisException: Unexpected exception while processing command\\n\\tat org.redisson.command.CommandAsyncService.convertException(CommandAsyncService.java:267)\\n\\tat org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:112)\\n\\tat org.redisson.RedissonObject.get(RedissonObject.java:55)\\n\\tat org.redisson.RedissonMapCache.scanIterator(RedissonMapCache.java:611)\\n\\tat org.redisson.RedissonMapIterator.iterator(RedissonMapIterator.java:32)\\n\\tat org.redisson.RedissonBaseMapIterator.hasNext(RedissonBaseMapIterator.java:66)\\n\\tat org.redisson.RedissonMapIterator.hasNext(RedissonMapIterator.java:23)\\n\\tat java.util.concurrent.ConcurrentMap.forEach(ConcurrentMap.java:104)\\n\\tat com.yudianbank.project.service.Impl.RedisServiceImpl.putClientRMapCache(RedisServiceImpl.java:132)\\n\\tat com.yudianbank.project.startup.InitRedisData.lambda$initAllClientData$5(InitRedisData.java:229)\\n\\tat java.util.HashMap.forEach(HashMap.java:1289)\\n\\tat com.yudianbank.project.startup.InitRedisData.initAllClientData(InitRedisData.java:228)\\n\\tat com.yudianbank.project.startup.InitRedisData.run(InitRedisData.java:107)\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:732)\\n\\t... 6 common frames omitted\\nCaused by: org.redisson.RedissonShutdownException: Redisson is shutdown\\n\\tat org.redisson.command.CommandAsyncService.async(CommandAsyncService.java:425)\\n\\tat org.redisson.command.CommandAsyncService.evalAsync(CommandAsyncService.java:401)\\n\\tat org.redisson.command.CommandAsyncService.evalReadAsync(CommandAsyncService.java:337)\\n\\tat org.redisson.RedissonMapCache.scanIterator(RedissonMapCache.java:537)\\n\\t... 16 common frames omitted\\n&quot;&#125; （三）使用tcp输出到logstash mdc默认添加到json输出的日志里 添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;net.logstash.logback&lt;/groupId&gt; &lt;artifactId&gt;logstash-logback-encoder&lt;/artifactId&gt; &lt;version&gt;4.11&lt;/version&gt;&lt;/dependency&gt; 配置logback.xml，加上以下内容（通过tcp协议发送到logstash，其他的方式参考 https://github.com/logstash/logstash-logback-encoder/tree/logstash-logback-encoder-4.11 ） 12345678&lt;appender name=&quot;stash&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt; &lt;destination&gt;127.0.0.1:10800&lt;/destination&gt; &lt;!-- encoder is required --&gt; &lt;encoder class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;&gt; &lt;includeCallerData&gt;true&lt;/includeCallerData&gt; &lt;customFields&gt;&#123;&quot;app_name&quot;:&quot;$&#123;appName&#125;&quot;&#125;&lt;/customFields&gt; &lt;/encoder&gt;&lt;/appender&gt; 启动项目，看到日志输出 123456789101112131415161718192021222324252627282930313233&#123; &quot;caller_file_name&quot; =&gt; &quot;DruidDataSource.java&quot;, &quot;level&quot; =&gt; &quot;INFO&quot;, &quot;caller_line_number&quot; =&gt; 1658, &quot;message&quot; =&gt; &quot;&#123;dataSource-1&#125; closed&quot;, &quot;type&quot; =&gt; &quot;docker&quot;, &quot;caller_method_name&quot; =&gt; &quot;close&quot;, &quot;@timestamp&quot; =&gt; 2017-11-16T08:39:49.543Z, &quot;app_name&quot; =&gt; &quot;authority&quot;, &quot;caller_class_name&quot; =&gt; &quot;com.alibaba.druid.pool.DruidDataSource&quot;, &quot;thread_name&quot; =&gt; &quot;Thread-69&quot;, &quot;level_value&quot; =&gt; 20000, &quot;@version&quot; =&gt; 1, &quot;host&quot; =&gt; &quot;172.17.0.1&quot;, &quot;logger_name&quot; =&gt; &quot;com.alibaba.druid.pool.DruidDataSource&quot;&#125;&#123; &quot;caller_file_name&quot; =&gt; &quot;SpringApplication.java&quot;, &quot;level&quot; =&gt; &quot;ERROR&quot;, &quot;caller_line_number&quot; =&gt; 771, &quot;message&quot; =&gt; &quot;Application startup failed&quot;, &quot;type&quot; =&gt; &quot;docker&quot;, &quot;caller_method_name&quot; =&gt; &quot;reportFailure&quot;, &quot;@timestamp&quot; =&gt; 2017-11-16T08:39:47.303Z, &quot;app_name&quot; =&gt; &quot;authority&quot;, &quot;caller_class_name&quot; =&gt; &quot;org.springframework.boot.SpringApplication&quot;, &quot;thread_name&quot; =&gt; &quot;main&quot;, &quot;level_value&quot; =&gt; 40000, &quot;@version&quot; =&gt; 1, &quot;host&quot; =&gt; &quot;172.17.0.1&quot;, &quot;logger_name&quot; =&gt; &quot;org.springframework.boot.SpringApplication&quot;, &quot;stack_trace&quot; =&gt; &quot;java.lang.IllegalStateException: Failed to execute CommandLineRunner\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:735)\\n\\tat org.springframework.boot.SpringApplication.callRunners(SpringApplication.java:716)\\n\\tat org.springframework.boot.SpringApplication.afterRefresh(SpringApplication.java:703)\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:304)\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1118)\\n\\tat org.springframework.boot.SpringApplication.run(SpringApplication.java:1107)\\n\\tat com.yudianbank.project.YdjfBootAuthorityApplication.main(YdjfBootAuthorityApplication.java:20)\\nCaused by: org.redisson.client.RedisException: Unexpected exception while processing command\\n\\tat org.redisson.command.CommandAsyncService.convertException(CommandAsyncService.java:267)\\n\\tat org.redisson.command.CommandAsyncService.get(CommandAsyncService.java:112)\\n\\tat org.redisson.RedissonObject.get(RedissonObject.java:55)\\n\\tat org.redisson.RedissonMapCache.scanIterator(RedissonMapCache.java:611)\\n\\tat org.redisson.RedissonMapIterator.iterator(RedissonMapIterator.java:32)\\n\\tat org.redisson.RedissonBaseMapIterator.hasNext(RedissonBaseMapIterator.java:66)\\n\\tat org.redisson.RedissonMapIterator.hasNext(RedissonMapIterator.java:23)\\n\\tat java.util.concurrent.ConcurrentMap.forEach(ConcurrentMap.java:104)\\n\\tat com.yudianbank.project.service.Impl.RedisServiceImpl.putClientRMapCache(RedisServiceImpl.java:132)\\n\\tat com.yudianbank.project.startup.InitRedisData.lambda$initAllClientData$5(InitRedisData.java:229)\\n\\tat java.util.HashMap.forEach(HashMap.java:1289)\\n\\tat com.yudianbank.project.startup.InitRedisData.initAllClientData(InitRedisData.java:228)\\n\\tat com.yudianbank.project.startup.InitRedisData.run(InitRedisData.java:107)\\n\\tat org.springframework.boot.SpringApplication.callRunner(SpringApplication.java:732)\\n\\t... 6 common frames omitted\\nCaused by: org.redisson.RedissonShutdownException: Redisson is shutdown\\n\\tat org.redisson.command.CommandAsyncService.async(CommandAsyncService.java:425)\\n\\tat org.redisson.command.CommandAsyncService.evalAsync(CommandAsyncService.java:401)\\n\\tat org.redisson.command.CommandAsyncService.evalReadAsync(CommandAsyncService.java:337)\\n\\tat org.redisson.RedissonMapCache.scanIterator(RedissonMapCache.java:537)\\n\\t... 16 common frames omitted\\n&quot;&#125; （四）access日志 每次请求的具体数据 能输出的数据 Field Description @timestamp Time of the log event. (yyyy-MM-dd’T’HH:mm:ss.SSSZZ) See customizing timezone. @version Logstash format version (e.g. 1) See customizing version. @message Message in the form ${remoteHost} - ${remoteUser} [${timestamp}] “${requestUrl}” ${statusCode} ${contentLength} @fields.method HTTP method @fields.protocol HTTP protocol @fields.status_code HTTP status code @fields.requested_url Request URL @fields.requested_uri Request URI @fields.remote_host Remote host @fields.HOSTNAME another field for remote host (not sure why this is here honestly) @fields.remote_user Remote user @fields.content_length Content length @fields.elapsed_time Elapsed time in millis （五）完整版配置参考123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;include resource=&quot;org/springframework/boot/logging/logback/defaults.xml&quot;/&gt; &lt;appender name=&quot;dailyRollingFile&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;File&gt;$&#123;logging.path&#125;/app.log&lt;/File&gt; &lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy&quot;&gt; &lt;!-- daily rollover --&gt; &lt;fileNamePattern&gt;$&#123;logging.path&#125;/app.%d&#123;yyyy-MM-dd&#125;.%i.log&lt;/fileNamePattern&gt; &lt;maxHistory&gt;30&lt;/maxHistory&gt; &lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt; &lt;/rollingPolicy&gt; &lt;!--&lt;rollingPolicy class=&quot;ch.qos.logback.core.rolling.TimeBasedRollingPolicy&quot;&gt;--&gt; &lt;!--&lt;fileNamePattern&gt;$&#123;logging.path&#125;/app-%d&#123;yyyyMMdd&#125;.%i.log&lt;/fileNamePattern&gt;--&gt; &lt;!--&lt;timeBasedFileNamingAndTriggeringPolicy class=&quot;ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP&quot;&gt;--&gt; &lt;!--&lt;maxFileSize&gt;50MB&lt;/maxFileSize&gt;--&gt; &lt;!--&lt;/timeBasedFileNamingAndTriggeringPolicy&gt;--&gt; &lt;!--&lt;maxHistory&gt;30&lt;/maxHistory&gt;--&gt; &lt;!--&lt;/rollingPolicy&gt;--&gt; &lt;encoder&gt; &lt;!-- %d&#123;HH:mm:ss.SSS&#125;——日志输出时间 %X&#123;traceId&#125;/%X&#123;spanId&#125;——预留 $&#123;appName&#125;——pom文件中定义的应用名 %thread——输出日志的进程名字，这在Web应用以及异步任务处理中很有用 %-5level——日志级别，并且使用5个字符靠左对齐 %logger&#123;36&#125;——日志输出者的名字 %msg——日志消息 %n——平台的换行符 --&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%X&#123;traceId&#125;/%X&#123;spanId&#125;] $&#123;appName&#125; [%thread] %-5level %logger&#123;35&#125; - %msg %n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=&quot;consoleRolling&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;$&#123;CONSOLE_LOG_PATTERN&#125;&lt;/pattern&gt; &lt;charset&gt;utf8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=&quot;stash&quot; class=&quot;net.logstash.logback.appender.LogstashSocketAppender&quot;&gt; &lt;port&gt;10800&lt;/port&gt;/ &lt;includeCallerData&gt;true&lt;/includeCallerData&gt; &lt;customFields&gt;&#123;&quot;app_name&quot;:&quot;authority&quot;&#125;&lt;/customFields&gt; &lt;/appender&gt; &lt;!--&lt;appender name=&quot;stash&quot; class=&quot;net.logstash.logback.appender.LogstashTcpSocketAppender&quot;&gt;--&gt; &lt;!--&lt;destination&gt;127.0.0.1:10800&lt;/destination&gt;--&gt; &lt;!--&amp;lt;!&amp;ndash; encoder is required &amp;ndash;&amp;gt;--&gt; &lt;!--&lt;encoder class=&quot;net.logstash.logback.encoder.LogstashEncoder&quot;&gt;--&gt; &lt;!--&lt;includeCallerData&gt;true&lt;/includeCallerData&gt;--&gt; &lt;!--&lt;customFields&gt;&#123;&quot;app_name&quot;:&quot;$&#123;appName&#125;&quot;&#125;&lt;/customFields&gt;--&gt; &lt;!--&lt;/encoder&gt;--&gt; &lt;!--&lt;/appender&gt;--&gt; &lt;logger name=&quot;com.yudianbank&quot; level=&quot;DEBUG&quot; additivity=&quot;false&quot;&gt; &lt;appender-ref ref=&quot;dailyRollingFile&quot;/&gt; &lt;appender-ref ref=&quot;consoleRolling&quot;/&gt; &lt;appender-ref ref=&quot;stash&quot;/&gt; &lt;/logger&gt; &lt;root level=&quot;INFO&quot;&gt; &lt;appender-ref ref=&quot;dailyRollingFile&quot;/&gt; &lt;appender-ref ref=&quot;consoleRolling&quot;/&gt; &lt;appender-ref ref=&quot;stash&quot;/&gt; &lt;/root&gt;&lt;/configuration&gt; 四. log4j2 日志输出前置 配置appName,在pom文件中增加 123&lt;properties&gt; &lt;appName&gt;authority&lt;/appName&gt;&lt;/properties&gt; 在.gitignore文件中添加忽略 1logs/ （一）配置日志本地输出 配置pom文件，确定输出路径 1234567&lt;!-- 线上环境 --&gt;&lt;!-- 日志存放地址 --&gt;&lt;logging.path&gt;/data/logs/$&#123;appName&#125;&lt;/logging.path&gt;&lt;!-- 本地环境 --&gt;&lt;!-- 日志存放地址 --&gt;&lt;logging.path&gt;$&#123;basedir&#125;/logs&lt;/logging.path&gt; log4j2.xml，添加console输出 1234567&lt;!-- 输出到控制台 --&gt;&lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot; ignoreExceptions=&quot;false&quot;&gt; &lt;!-- 需要记录的级别 --&gt; &lt;ThresholdFilter level=&quot;debug&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;!--&lt;PatternLayout pattern=&quot;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;:%4p %t (%F:%L) - %m%n&quot;/&gt;--&gt; &lt;PatternLayout pattern=&quot;%d [%X&#123;traceId&#125;/%X&#123;spanId&#125;] [%thread] %-5level %logger&#123;36&#125; - %msg%n&quot;/&gt;&lt;/Console&gt; 配置log4j2.xml，添加本地日志输出 123456789101112&lt;RollingFile name=&quot;dailyRollingFile&quot; immediateFlush=&quot;true&quot; fileName=&quot;$&#123;logging.path&#125;/app.log&quot; filePattern=&quot;$&#123;logging.path&#125;/app.%d&#123;yyyy-MM-dd&#125;.%i.log&quot;&gt; &lt;PatternLayout&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%X&#123;traceId&#125;/%X&#123;spanId&#125;] $&#123;appName&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;OnStartupTriggeringPolicy/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;!-- 每个日志文件最大50MB --&gt; &lt;SizeBasedTriggeringPolicy size=&quot;50MB&quot; /&gt; &lt;/Policies&gt;&lt;/RollingFile&gt; 启动项目，看到日志输出 日志文件输出 123456789101112131413:48:07.950 [/] weixin [RMI TCP Connection(4)-127.0.0.1] INFO com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener - 队列boot.queue.com.yudianbank.api.service.api.ApiToFrontSerive创建成功！13:48:08.064 [/] weixin [RMI TCP Connection(4)-127.0.0.1] INFO com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener - 队列boot.queue.com.yudianbank.api.service.api.ApiToApproveService创建成功！13:48:08.120 [/] weixin [RMI TCP Connection(4)-127.0.0.1] INFO com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener - 队列boot.queue.com.yudianbank.api.service.AppMgmToWeixinService创建成功！13:48:08.703 [/] weixin [RMI TCP Connection(4)-127.0.0.1] INFO com.yudianbank.common.config.RedissonConfigration - redisson.yaml &gt;&gt; content:&#123;&quot;singleServerConfig&quot;:&#123;&quot;idleConnectionTimeout&quot;:10000,&quot;pingTimeout&quot;:1000,&quot;connectTimeout&quot;:10000,&quot;timeout&quot;:3000,&quot;retryAttempts&quot;:3,&quot;retryInterval&quot;:1500,&quot;reconnectionTimeout&quot;:3000,&quot;failedAttempts&quot;:3,&quot;subscriptionsPerConnection&quot;:5,&quot;address&quot;:[&quot;//192.168.1.204:6379&quot;],&quot;subscriptionConnectionMinimumIdleSize&quot;:1,&quot;subscriptionConnectionPoolSize&quot;:50,&quot;connectionMinimumIdleSize&quot;:10,&quot;connectionPoolSize&quot;:64,&quot;database&quot;:0,&quot;dnsMonitoring&quot;:false,&quot;dnsMonitoringInterval&quot;:5000&#125;,&quot;threads&quot;:0,&quot;codec&quot;:&#123;&quot;class&quot;:&quot;org.redisson.codec.JsonJacksonCodec&quot;&#125;,&quot;useLinuxNativeEpoll&quot;:false&#125;13:48:13.204 [/] weixin [http-nio-8080-exec-1] INFO com.yudianbank.common.web.filter.BackURLFilter - http://localhost:8080/ [GET]13:48:13.218 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - Request IP: 127.0.0.113:48:13.221 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - Request Header:13:48:13.222 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - user-agent = IntelliJ IDEA/172.4343.1413:48:13.223 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - accept-encoding = gzip13:48:13.224 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - cache-control = no-cache13:48:13.225 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - pragma = no-cache13:48:13.226 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - host = localhost:808013:48:13.227 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - accept = text/html, image/gif, image/jpeg, *; q=.2, */*; q=.213:48:13.228 [/] weixin [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - connection = keep-alive console输出 123456789101112131415162017-11-22 13:50:55.314 [/] [RMI TCP Connection(2)-127.0.0.1] INFO com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener - 队列boot.queue.com.yudianbank.api.service.api.ApiToFrontSerive创建成功！2017-11-22 13:50:55.469 [/] [RMI TCP Connection(2)-127.0.0.1] INFO com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener - 队列boot.queue.com.yudianbank.api.service.api.ApiToApproveService创建成功！2017-11-22 13:50:55.510 [/] [RMI TCP Connection(2)-127.0.0.1] INFO com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener - 队列boot.queue.com.yudianbank.api.service.AppMgmToWeixinService创建成功！2017-11-22 13:50:56.025 [/] [RMI TCP Connection(2)-127.0.0.1] INFO com.yudianbank.common.config.RedissonConfigration - redisson.yaml &gt;&gt; content:&#123;&quot;singleServerConfig&quot;:&#123;&quot;idleConnectionTimeout&quot;:10000,&quot;pingTimeout&quot;:1000,&quot;connectTimeout&quot;:10000,&quot;timeout&quot;:3000,&quot;retryAttempts&quot;:3,&quot;retryInterval&quot;:1500,&quot;reconnectionTimeout&quot;:3000,&quot;failedAttempts&quot;:3,&quot;subscriptionsPerConnection&quot;:5,&quot;address&quot;:[&quot;//192.168.1.204:6379&quot;],&quot;subscriptionConnectionMinimumIdleSize&quot;:1,&quot;subscriptionConnectionPoolSize&quot;:50,&quot;connectionMinimumIdleSize&quot;:10,&quot;connectionPoolSize&quot;:64,&quot;database&quot;:0,&quot;dnsMonitoring&quot;:false,&quot;dnsMonitoringInterval&quot;:5000&#125;,&quot;threads&quot;:0,&quot;codec&quot;:&#123;&quot;class&quot;:&quot;org.redisson.codec.JsonJacksonCodec&quot;&#125;,&quot;useLinuxNativeEpoll&quot;:false&#125;[2017-11-22 01:51:00,500] Artifact root-weixin:war: Artifact is deployed successfully[2017-11-22 01:51:00,500] Artifact root-weixin:war: Deploy took 27,783 milliseconds2017-11-22 13:51:00.998 [/] [http-nio-8080-exec-1] INFO com.yudianbank.common.web.filter.BackURLFilter - http://localhost:8080/ [GET]2017-11-22 13:51:01.010 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - Request IP: 127.0.0.12017-11-22 13:51:01.011 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - Request Header:2017-11-22 13:51:01.011 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - user-agent = IntelliJ IDEA/172.4343.142017-11-22 13:51:01.012 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - accept-encoding = gzip2017-11-22 13:51:01.012 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - cache-control = no-cache2017-11-22 13:51:01.012 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - pragma = no-cache2017-11-22 13:51:01.012 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - host = localhost:80802017-11-22 13:51:01.012 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - accept = text/html, image/gif, image/jpeg, *; q=.2, */*; q=.22017-11-22 13:51:01.012 [/] [http-nio-8080-exec-1] DEBUG com.yudianbank.common.web.filter.BackURLFilter - connection = keep-alive （二）使用gelf输出到logstash mdc默认添加到json输出的日志里 添加依赖 123456&lt;!-- logstash:UDP --&gt;&lt;dependency&gt; &lt;groupId&gt;biz.paluch.logging&lt;/groupId&gt; &lt;artifactId&gt;logstash-gelf&lt;/artifactId&gt; &lt;version&gt;1.11.1&lt;/version&gt;&lt;/dependency&gt; 配置log4j2.xml，加上以下内容 1234567891011121314151617181920212223&lt;!--log4j2 logstash-gelf UDP 协议，自动重连 https://github.com/mp911de/logstash-gelf--&gt;&lt;Gelf name=&quot;gelf&quot; host=&quot;udp:localhost&quot; port=&quot;12201&quot; version=&quot;1.1&quot; extractStackTrace=&quot;true&quot; filterStackTrace=&quot;true&quot; mdcProfiling=&quot;true&quot; includeFullMdc=&quot;true&quot; maximumMessageSize=&quot;8192&quot; originHost=&quot;%host&#123;fqdn&#125;&quot; additionalFieldTypes=&quot;fieldName1=String,fieldName2=String&quot;&gt; &lt;Field name=&quot;timestamp&quot; pattern=&quot;%d&#123;dd MMM yyyy HH:mm:ss.SSS&#125;&quot;/&gt; &lt;Field name=&quot;level&quot; pattern=&quot;%level&quot;/&gt; &lt;Field name=&quot;caller_class_name&quot; pattern=&quot;%C&quot;/&gt; &lt;Field name=&quot;caller_line_number&quot; pattern=&quot;%L&quot;/&gt; &lt;Field name=&quot;caller_method_name&quot; pattern=&quot;%M&quot;/&gt; &lt;Field name=&quot;logger_name&quot; pattern=&quot;%c&quot;/&gt; &lt;Field name=&quot;thread_name&quot; pattern=&quot;%t&quot;/&gt; &lt;!--&lt;Field name=&quot;className&quot; pattern=&quot;%C&quot;/&gt;--&gt; &lt;Field name=&quot;server&quot; pattern=&quot;%host&quot;/&gt; &lt;Field name=&quot;server.fqdn&quot; pattern=&quot;%host&#123;fqdn&#125;&quot;/&gt; &lt;!-- This is a static field --&gt; &lt;!--&lt;Field name=&quot;fieldName2&quot; literal=&quot;fieldValue2&quot; /&gt;--&gt; &lt;!-- This is a field using MDC --&gt; &lt;!--&lt;Field name=&quot;traceId&quot; mdc=&quot;traceId&quot;/&gt;--&gt; &lt;!--&lt;Field name=&quot;spanId&quot; mdc=&quot;spanId&quot;/&gt;--&gt; &lt;Field name=&quot;app_name&quot; mdc=&quot;$&#123;appName&#125;&quot;/&gt; &lt;DynamicMdcFields regex=&quot;mdc.*&quot;/&gt; &lt;DynamicMdcFields regex=&quot;(mdc|MDC)fields&quot;/&gt;&lt;/Gelf&gt; 启动项目，看到日志输出 123456789101112131415161718192021222324252627282930313233343536&#123; &quot;server&quot; =&gt; &quot;localhost&quot;, &quot;source_host&quot; =&gt; &quot;172.17.0.1&quot;, &quot;level&quot; =&gt; &quot;INFO&quot;, &quot;caller_line_number&quot; =&gt; 147, &quot;message&quot; =&gt; &quot;队列boot.queue.com.yudianbank.api.service.api.ApiToApproveService创建成功！&quot;, &quot;type&quot; =&gt; &quot;docker&quot;, &quot;server.fqdn&quot; =&gt; &quot;localhost&quot;, &quot;caller_method_name&quot; =&gt; &quot;afterPropertiesSet&quot;, &quot;@timestamp&quot; =&gt; 2017-11-22T07:08:02.822Z, &quot;caller_class_name&quot; =&gt; &quot;com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener&quot;, &quot;thread_name&quot; =&gt; &quot;RMI TCP Connection(2)-127.0.0.1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;logger_name&quot; =&gt; &quot;com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener&quot;, &quot;facility&quot; =&gt; &quot;logstash-gelf&quot;, &quot;timestamp&quot; =&gt; &quot;22 十一月 2017 15:08:02.822&quot;&#125;&#123; &quot;server&quot; =&gt; &quot;localhost&quot;, &quot;source_host&quot; =&gt; &quot;172.17.0.1&quot;, &quot;level&quot; =&gt; &quot;INFO&quot;, &quot;caller_line_number&quot; =&gt; 147, &quot;message&quot; =&gt; &quot;队列boot.queue.com.yudianbank.api.service.AppMgmToWeixinService创建成功！&quot;, &quot;type&quot; =&gt; &quot;docker&quot;, &quot;server.fqdn&quot; =&gt; &quot;localhost&quot;, &quot;caller_method_name&quot; =&gt; &quot;afterPropertiesSet&quot;, &quot;@timestamp&quot; =&gt; 2017-11-22T07:08:02.858Z, &quot;caller_class_name&quot; =&gt; &quot;com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener&quot;, &quot;thread_name&quot; =&gt; &quot;RMI TCP Connection(2)-127.0.0.1&quot;, &quot;host&quot; =&gt; &quot;localhost&quot;, &quot;@version&quot; =&gt; &quot;1&quot;, &quot;logger_name&quot; =&gt; &quot;com.yudianbank.framework.rpc.sdk.ConsumerInvokerListener&quot;, &quot;facility&quot; =&gt; &quot;logstash-gelf&quot;, &quot;timestamp&quot; =&gt; &quot;22 十一月 2017 15:08:02.858&quot;&#125; （三）完整版配置参考123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;!-- Log4j 2.x 配置文件。每30秒自动检查和应用配置文件的更新； --&gt;&lt;Configuration status=&quot;WARN&quot; monitorInterval=&quot;30&quot; strict=&quot;true&quot; schema=&quot;Log4J-V2.2.xsd&quot;&gt; &lt;Appenders&gt; &lt;!-- 输出到控制台 --&gt; &lt;Console name=&quot;Console&quot; target=&quot;SYSTEM_OUT&quot; ignoreExceptions=&quot;false&quot;&gt; &lt;!-- 需要记录的级别 --&gt; &lt;ThresholdFilter level=&quot;debug&quot; onMatch=&quot;ACCEPT&quot; onMismatch=&quot;DENY&quot;/&gt; &lt;!--&lt;PatternLayout pattern=&quot;%d&#123;yyyy-MM-dd HH:mm:ss,SSS&#125;:%4p %t (%F:%L) - %m%n&quot;/&gt;--&gt; &lt;PatternLayout pattern=&quot;%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; [%X&#123;traceId&#125;/%X&#123;spanId&#125;] [%thread] %-5level %logger&#123;36&#125; - %msg%n&quot;/&gt; &lt;/Console&gt; &lt;RollingFile name=&quot;dailyRollingFile&quot; immediateFlush=&quot;true&quot; fileName=&quot;$&#123;logging.path&#125;/app.log&quot; filePattern=&quot;$&#123;logging.path&#125;/app.%d&#123;yyyy-MM-dd&#125;.%i.log&quot;&gt; &lt;PatternLayout&gt; &lt;pattern&gt;%d&#123;HH:mm:ss.SSS&#125; [%X&#123;traceId&#125;/%X&#123;spanId&#125;] $&#123;appName&#125; [%thread] %-5level %logger&#123;36&#125; - %msg%n&lt;/pattern&gt; &lt;/PatternLayout&gt; &lt;Policies&gt; &lt;OnStartupTriggeringPolicy/&gt; &lt;TimeBasedTriggeringPolicy/&gt; &lt;!-- 每个日志文件最大50MB --&gt; &lt;SizeBasedTriggeringPolicy size=&quot;50MB&quot; /&gt; &lt;/Policies&gt; &lt;/RollingFile&gt; &lt;!-- 输出到文件，按天或者超过80MB分割 logstash 每隔2秒去读取 验证 ok --&gt; &lt;!--&lt;RollingFile name=&quot;RollingFile&quot; fileName=&quot;/data/logs/logstash.log&quot;--&gt; &lt;!--filePattern=&quot;/data/logs/$$&#123;date:yyyy-MM&#125;/logstash-%d&#123;yyyy-MM-dd&#125;-%i.log.gz&quot;--&gt; &lt;!--ignoreExceptions=&quot;false&quot;&gt;--&gt; &lt;!--&lt;JSONLayout complete=&quot;true&quot; includeStacktrace=&quot;true&quot; locationInfo=&quot;true&quot;/&gt;--&gt; &lt;!--&lt;Policies&gt;--&gt; &lt;!--&lt;OnStartupTriggeringPolicy/&gt;--&gt; &lt;!--&lt;TimeBasedTriggeringPolicy/&gt;--&gt; &lt;!--&lt;SizeBasedTriggeringPolicy size=&quot;80 MB&quot;/&gt;--&gt; &lt;!--&lt;/Policies&gt;--&gt; &lt;!--&lt;/RollingFile&gt;--&gt; &lt;!-- 输出到 logstash,通过 TCP 协议, 这种方式有个缺点是断掉之后不会自动重连 验证 ok Logstash提供了log4j输入插件，但是只能用于log4j1.x，不能用于log4j2，因此，我们在配置文件中使用tcp输入插件，关于该插件的参数解释， 详见：https://www.elastic.co/guide/en/logstash/current/plugins-inputs-tcp.html。 --&gt; &lt;!--&lt;Socket name=&quot;logstash&quot; host=&quot;127.0.0.1&quot; port=&quot;5460&quot; protocol=&quot;TCP&quot;&gt;--&gt; &lt;!--&lt;LogStashJSONLayout&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;application_name&quot; value=&quot;$&#123;sys:application.name&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;application_version&quot; value=&quot;$&#123;sys:application.version&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;environment_type&quot; value=&quot;$&#123;sys:deploy_env&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;cluster_location&quot; value=&quot;$&#123;sys:cluster_location&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;cluster_name&quot; value=&quot;$&#123;sys:cluster_name&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;hostname&quot; value=&quot;$&#123;sys:hostname&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;host_ip&quot; value=&quot;$&#123;sys:host_ip&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;application_user&quot; value=&quot;$&#123;sys:user.name&#125;&quot;/&gt;--&gt; &lt;!--&lt;KeyValuePair key=&quot;environment_user&quot; value=&quot;$&#123;env:USER&#125;&quot;/&gt;--&gt; &lt;!--&lt;/LogStashJSONLayout&gt;--&gt; &lt;!--&lt;/Socket&gt;--&gt; &lt;!-- 直接输出到 kafka 成功，验证 ok --&gt; &lt;!--&lt;Kafka name=&quot;Kafka&quot; topic=&quot;test_stash&quot;&gt;--&gt; &lt;!--&lt;JSONLayout complete=&quot;true&quot; includeStacktrace=&quot;true&quot; locationInfo=&quot;true&quot; compact=&quot;true&quot;/&gt;--&gt; &lt;!--&lt;Property name=&quot;bootstrap.servers&quot;&gt;192.168.1.204:9092&lt;/Property&gt;--&gt; &lt;!--&lt;/Kafka&gt;--&gt; &lt;!--log4j2 logstash-gelf UDP 协议，自动重连 https://github.com/mp911de/logstash-gelf--&gt; &lt;Gelf name=&quot;gelf&quot; host=&quot;udp:localhost&quot; port=&quot;12201&quot; version=&quot;1.1&quot; extractStackTrace=&quot;true&quot; filterStackTrace=&quot;true&quot; mdcProfiling=&quot;true&quot; includeFullMdc=&quot;true&quot; maximumMessageSize=&quot;8192&quot; originHost=&quot;%host&#123;fqdn&#125;&quot; additionalFieldTypes=&quot;fieldName1=String,fieldName2=String&quot;&gt; &lt;Field name=&quot;timestamp&quot; pattern=&quot;%d&#123;dd MMM yyyy HH:mm:ss.SSS&#125;&quot;/&gt; &lt;Field name=&quot;level&quot; pattern=&quot;%level&quot;/&gt; &lt;Field name=&quot;caller_class_name&quot; pattern=&quot;%C&quot;/&gt; &lt;Field name=&quot;caller_line_number&quot; pattern=&quot;%L&quot;/&gt; &lt;Field name=&quot;caller_method_name&quot; pattern=&quot;%M&quot;/&gt; &lt;Field name=&quot;logger_name&quot; pattern=&quot;%c&quot;/&gt; &lt;Field name=&quot;thread_name&quot; pattern=&quot;%t&quot;/&gt; &lt;!--&lt;Field name=&quot;className&quot; pattern=&quot;%C&quot;/&gt;--&gt; &lt;Field name=&quot;server&quot; pattern=&quot;%host&quot;/&gt; &lt;Field name=&quot;server.fqdn&quot; pattern=&quot;%host&#123;fqdn&#125;&quot;/&gt; &lt;!-- This is a static field --&gt; &lt;!--&lt;Field name=&quot;fieldName2&quot; literal=&quot;fieldValue2&quot; /&gt;--&gt; &lt;!-- This is a field using MDC --&gt; &lt;!--&lt;Field name=&quot;traceId&quot; mdc=&quot;traceId&quot;/&gt;--&gt; &lt;!--&lt;Field name=&quot;spanId&quot; mdc=&quot;spanId&quot;/&gt;--&gt; &lt;Field name=&quot;app_name&quot; mdc=&quot;$&#123;appName&#125;&quot;/&gt; &lt;DynamicMdcFields regex=&quot;mdc.*&quot;/&gt; &lt;DynamicMdcFields regex=&quot;(mdc|MDC)fields&quot;/&gt; &lt;/Gelf&gt; &lt;/Appenders&gt; &lt;Loggers&gt; &lt;Root level=&quot;FATAL&quot;&gt; &lt;!-- 全局配置 ALL &gt; TRACE &gt; DEBUG &gt; INFO &gt; WARN &gt; ERROR &gt; FATAL &gt; OFF --&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;AppenderRef ref=&quot;dailyRollingFile&quot;/&gt; &lt;/Root&gt; &lt;Logger name=&quot;com.yudianbank&quot; level=&quot;ALL&quot; additivity=&quot;false&quot;&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;AppenderRef ref=&quot;gelf&quot;/&gt; &lt;AppenderRef ref=&quot;dailyRollingFile&quot;/&gt; &lt;/Logger&gt; &lt;!-- 为sql语句配置特殊的Log级别，方便调试 --&gt; &lt;Logger name=&quot;com.p6spy&quot; level=&quot;INFO&quot; additivity=&quot;false&quot;&gt; &lt;AppenderRef ref=&quot;Console&quot;/&gt; &lt;AppenderRef ref=&quot;dailyRollingFile&quot;/&gt; &lt;/Logger&gt; &lt;/Loggers&gt;&lt;/Configuration&gt;","categories":[],"tags":[{"name":"log4j2","slug":"log4j2","permalink":"https://engining.net/tags/log4j2/"},{"name":"logback","slug":"logback","permalink":"https://engining.net/tags/logback/"},{"name":"docker","slug":"docker","permalink":"https://engining.net/tags/docker/"},{"name":"kafka","slug":"kafka","permalink":"https://engining.net/tags/kafka/"},{"name":"logstash","slug":"logstash","permalink":"https://engining.net/tags/logstash/"}],"author":"凯京.朱振辉"}],"categories":[{"name":"hexo theme","slug":"hexo-theme","permalink":"https://engining.net/categories/hexo-theme/"},{"name":"hexo","slug":"hexo","permalink":"https://engining.net/categories/hexo/"}],"tags":[{"name":"pure","slug":"pure","permalink":"https://engining.net/tags/pure/"},{"name":"hexo","slug":"hexo","permalink":"https://engining.net/tags/hexo/"},{"name":"quick start","slug":"quick-start","permalink":"https://engining.net/tags/quick-start/"},{"name":"mysql master-master binlog","slug":"mysql-master-master-binlog","permalink":"https://engining.net/tags/mysql-master-master-binlog/"},{"name":"云原生","slug":"云原生","permalink":"https://engining.net/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"},{"name":"微服务","slug":"微服务","permalink":"https://engining.net/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"},{"name":"servicemesh","slug":"servicemesh","permalink":"https://engining.net/tags/servicemesh/"},{"name":"容器化","slug":"容器化","permalink":"https://engining.net/tags/%E5%AE%B9%E5%99%A8%E5%8C%96/"},{"name":"aliyun","slug":"aliyun","permalink":"https://engining.net/tags/aliyun/"},{"name":"日志组件","slug":"日志组件","permalink":"https://engining.net/tags/%E6%97%A5%E5%BF%97%E7%BB%84%E4%BB%B6/"},{"name":"livetail","slug":"livetail","permalink":"https://engining.net/tags/livetail/"},{"name":"k8s","slug":"k8s","permalink":"https://engining.net/tags/k8s/"},{"name":"springboot @Enable*","slug":"springboot-Enable","permalink":"https://engining.net/tags/springboot-Enable/"},{"name":"分布式缓存","slug":"分布式缓存","permalink":"https://engining.net/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"Redis","slug":"Redis","permalink":"https://engining.net/tags/Redis/"},{"name":"memcache","slug":"memcache","permalink":"https://engining.net/tags/memcache/"},{"name":"guavacache","slug":"guavacache","permalink":"https://engining.net/tags/guavacache/"},{"name":"ehcache","slug":"ehcache","permalink":"https://engining.net/tags/ehcache/"},{"name":"tair","slug":"tair","permalink":"https://engining.net/tags/tair/"},{"name":"DevOps","slug":"DevOps","permalink":"https://engining.net/tags/DevOps/"},{"name":"大盘","slug":"大盘","permalink":"https://engining.net/tags/%E5%A4%A7%E7%9B%98/"},{"name":"持续交付","slug":"持续交付","permalink":"https://engining.net/tags/%E6%8C%81%E7%BB%AD%E4%BA%A4%E4%BB%98/"},{"name":"Aspectj","slug":"Aspectj","permalink":"https://engining.net/tags/Aspectj/"},{"name":"aop","slug":"aop","permalink":"https://engining.net/tags/aop/"},{"name":"spring","slug":"spring","permalink":"https://engining.net/tags/spring/"},{"name":"开源协议","slug":"开源协议","permalink":"https://engining.net/tags/%E5%BC%80%E6%BA%90%E5%8D%8F%E8%AE%AE/"},{"name":"开源","slug":"开源","permalink":"https://engining.net/tags/%E5%BC%80%E6%BA%90/"},{"name":"在线预览","slug":"在线预览","permalink":"https://engining.net/tags/%E5%9C%A8%E7%BA%BF%E9%A2%84%E8%A7%88/"},{"name":"office","slug":"office","permalink":"https://engining.net/tags/office/"},{"name":"zip","slug":"zip","permalink":"https://engining.net/tags/zip/"},{"name":"pdf","slug":"pdf","permalink":"https://engining.net/tags/pdf/"},{"name":"log4j2","slug":"log4j2","permalink":"https://engining.net/tags/log4j2/"},{"name":"logback","slug":"logback","permalink":"https://engining.net/tags/logback/"},{"name":"docker","slug":"docker","permalink":"https://engining.net/tags/docker/"},{"name":"kafka","slug":"kafka","permalink":"https://engining.net/tags/kafka/"},{"name":"logstash","slug":"logstash","permalink":"https://engining.net/tags/logstash/"}]}